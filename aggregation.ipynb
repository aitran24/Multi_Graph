{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5affdfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: d:\\nckh\\auditlog\\output\n",
      "Output directory: d:\\nckh\\auditlog\\output_aggregated\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "BASE_DIR = Path(r'd:\\nckh\\auditlog')\n",
    "INPUT_DIR = BASE_DIR / 'output'\n",
    "OUTPUT_DIR = BASE_DIR / 'output_aggregated'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6250a",
   "metadata": {},
   "source": [
    "## Step 1: Path Generalization\n",
    "\n",
    "Bi·∫øn ƒë∆∞·ªùng d·∫´n c·ª• th·ªÉ th√†nh environment variables ƒë·ªÉ graph c√≥ th·ªÉ match v·ªõi nhi·ªÅu m√°y kh√°c nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9667cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: C:\\Users\\Admin\\AppData\\Local\\Temp\\test.txt\n",
      "Generalized: %USERPROFILE%%TEMP%\\test.txt\n",
      "\n",
      "Original: C:\\Windows\\System32\\cmd.exe\n",
      "Generalized: %WINDIR%\\System32\\cmd.exe\n",
      "\n",
      "Original: C:\\ProgramData\\Microsoft\\Windows\\Start Menu\n",
      "Generalized: %PROGRAMDATA%\\Microsoft\\Windows\\Start Menu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generalize_path(path_str):\n",
    "    \"\"\"\n",
    "    Generalize Windows paths to use environment variables.\n",
    "    \n",
    "    Examples:\n",
    "        C:\\\\Users\\\\Admin\\\\AppData ‚Üí %USERPROFILE%\\\\AppData\n",
    "        C:\\\\Windows\\\\System32 ‚Üí %WINDIR%\\\\System32\n",
    "        C:\\\\ProgramData ‚Üí %PROGRAMDATA%\n",
    "    \"\"\"\n",
    "    if not isinstance(path_str, str):\n",
    "        return path_str\n",
    "    \n",
    "    original = path_str\n",
    "    \n",
    "    # Case-insensitive replacements\n",
    "    patterns = [\n",
    "        (r'C:\\\\Users\\\\[^\\\\]+', r'%USERPROFILE%'),  # User profile\n",
    "        (r'C:\\\\Windows', r'%WINDIR%'),  # Windows directory\n",
    "        (r'C:\\\\ProgramData', r'%PROGRAMDATA%'),  # ProgramData\n",
    "        (r'C:\\\\Program Files( \\(x86\\))?', r'%PROGRAMFILES%'),  # Program Files\n",
    "        (r'\\\\AppData\\\\Local\\\\Temp', r'%TEMP%'),  # Temp (after USERPROFILE)\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in patterns:\n",
    "        path_str = re.sub(pattern, replacement, path_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    return path_str\n",
    "\n",
    "\n",
    "# Test\n",
    "test_paths = [\n",
    "    r\"C:\\Users\\Admin\\AppData\\Local\\Temp\\test.txt\",\n",
    "    r\"C:\\Windows\\System32\\cmd.exe\",\n",
    "    r\"C:\\ProgramData\\Microsoft\\Windows\\Start Menu\",\n",
    "]\n",
    "\n",
    "for path in test_paths:\n",
    "    print(f\"Original: {path}\")\n",
    "    print(f\"Generalized: {generalize_path(path)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497ceea",
   "metadata": {},
   "source": [
    "## Step 2: Noise Filtering Rules\n",
    "\n",
    "ƒê·ªãnh nghƒ©a c√°c patterns c·∫ßn lo·∫°i b·ªè d·ª±a tr√™n ph√¢n t√≠ch noise t·ª´ c√°c techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360cd9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Noise filtering rules loaded\n"
     ]
    }
   ],
   "source": [
    "# Noise patterns to filter (based on previous analysis)\n",
    "NOISE_PATTERNS = {\n",
    "    'file': [\n",
    "        r'PSScriptPolicyTest',\n",
    "        r'art-err\\.txt',\n",
    "        r'art-out\\.txt',\n",
    "        r'\\\\Temp\\\\tmp[A-F0-9]+\\.tmp',  # Temporary files\n",
    "        r'__PSScriptPolicyTest',\n",
    "        r'\\.etl$',  # ETW log files\n",
    "    ],\n",
    "    'process': [\n",
    "        r'^chcp\\.com$',\n",
    "        r'^conhost\\.exe$',\n",
    "    ],\n",
    "    'registry': [\n",
    "        r'Software\\\\Microsoft\\\\PowerShell\\\\1\\\\ShellIds',\n",
    "    ],\n",
    "    'operation': [\n",
    "        'DELETE_FILE',\n",
    "        'DELETE_REGISTRY',\n",
    "    ]\n",
    "}\n",
    "\n",
    "def is_noise_node(node_data):\n",
    "    \"\"\"\n",
    "    Check if a node should be filtered as noise.\n",
    "    \"\"\"\n",
    "    node_type = node_data.get('type', '')\n",
    "    \n",
    "    # Check file paths\n",
    "    if node_type == 'File':\n",
    "        path = node_data.get('properties', {}).get('path', '')\n",
    "        for pattern in NOISE_PATTERNS['file']:\n",
    "            if re.search(pattern, path, re.IGNORECASE):\n",
    "                return True\n",
    "    \n",
    "    # Check process names\n",
    "    if node_type == 'Process':\n",
    "        image = node_data.get('properties', {}).get('image', '')\n",
    "        label = node_data.get('properties', {}).get('label', '')\n",
    "        for pattern in NOISE_PATTERNS['process']:\n",
    "            if re.search(pattern, image, re.IGNORECASE) or re.search(pattern, label, re.IGNORECASE):\n",
    "                return True\n",
    "    \n",
    "    # Check registry paths\n",
    "    if node_type == 'Registry':\n",
    "        key = node_data.get('properties', {}).get('key', '')\n",
    "        for pattern in NOISE_PATTERNS['registry']:\n",
    "            if re.search(pattern, key, re.IGNORECASE):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def is_noise_edge(edge_data):\n",
    "    \"\"\"\n",
    "    Check if an edge should be filtered as noise.\n",
    "    \"\"\"\n",
    "    operations = edge_data.get('operations', [])\n",
    "    \n",
    "    # Filter DELETE operations\n",
    "    for op in operations:\n",
    "        if any(noise_op in op for noise_op in NOISE_PATTERNS['operation']):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"‚úì Noise filtering rules loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a971967",
   "metadata": {},
   "source": [
    "## Step 3: Re-identification Strategy\n",
    "\n",
    "Thay th·∫ø GUID ng·∫´u nhi√™n b·∫±ng stable identifiers d·ª±a tr√™n n·ªôi dung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003e7549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ID: Process:{D28789B6-7C64-5FA1-0C00-000000008801}\n",
      "Stable ID: Process:powershell|63c1a3f4\n"
     ]
    }
   ],
   "source": [
    "def compute_content_hash(text, length=8):\n",
    "    \"\"\"\n",
    "    Compute a short hash of text content.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(text.encode()).hexdigest()[:length]\n",
    "\n",
    "\n",
    "def create_stable_id(node_data):\n",
    "    \"\"\"\n",
    "    Create a stable, content-based ID for a node.\n",
    "    \n",
    "    Examples:\n",
    "        Process:{GUID} ‚Üí Process:powershell.exe|<command_hash>\n",
    "        File:{GUID} ‚Üí File:<path_hash>\n",
    "        Registry:{GUID} ‚Üí Registry:<key_hash>\n",
    "    \"\"\"\n",
    "    node_type = node_data.get('type', 'Unknown')\n",
    "    props = node_data.get('properties', {})\n",
    "    \n",
    "    if node_type == 'Process':\n",
    "        label = props.get('label', 'unknown')\n",
    "        command = props.get('commandLine', props.get('image', ''))\n",
    "        cmd_hash = compute_content_hash(command)\n",
    "        return f\"Process:{label}|{cmd_hash}\"\n",
    "    \n",
    "    elif node_type == 'File':\n",
    "        path = props.get('path', 'unknown')\n",
    "        path_hash = compute_content_hash(path)\n",
    "        return f\"File:{path_hash}\"\n",
    "    \n",
    "    elif node_type == 'Registry':\n",
    "        key = props.get('key', 'unknown')\n",
    "        key_hash = compute_content_hash(key)\n",
    "        return f\"Registry:{key_hash}\"\n",
    "    \n",
    "    elif node_type == 'Image':\n",
    "        path = props.get('path', 'unknown')\n",
    "        path_hash = compute_content_hash(path)\n",
    "        return f\"Image:{path_hash}\"\n",
    "    \n",
    "    else:\n",
    "        # Fallback\n",
    "        return node_data.get('id', f\"{node_type}:unknown\")\n",
    "\n",
    "\n",
    "# Test\n",
    "test_node = {\n",
    "    'id': 'Process:{D28789B6-7C64-5FA1-0C00-000000008801}',\n",
    "    'type': 'Process',\n",
    "    'properties': {\n",
    "        'label': 'powershell',\n",
    "        'commandLine': 'powershell.exe -c \"reg add HKCU\\\\Software\\\\Test\"'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Original ID: {test_node['id']}\")\n",
    "print(f\"Stable ID: {create_stable_id(test_node)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82947e6b",
   "metadata": {},
   "source": [
    "## Step 4: Node Merging Logic\n",
    "\n",
    "G·ªôp c√°c nodes c√≥ h√†nh vi gi·ªëng nhau ƒë·ªÉ compact graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448c532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Node merging logic ready\n"
     ]
    }
   ],
   "source": [
    "def compute_node_signature(node_data):\n",
    "    \"\"\"\n",
    "    Compute a signature for node merging.\n",
    "    Nodes with same signature can be merged.\n",
    "    \"\"\"\n",
    "    node_type = node_data.get('type')\n",
    "    props = node_data.get('properties', {})\n",
    "    \n",
    "    if node_type == 'Process':\n",
    "        # Merge based on process name + command pattern\n",
    "        label = props.get('label', '')\n",
    "        command = props.get('commandLine', '')\n",
    "        # Generalize command (remove paths)\n",
    "        command_pattern = re.sub(r'[A-Z]:\\\\[^\\s\"]+', '<PATH>', command, flags=re.IGNORECASE)\n",
    "        return f\"{node_type}:{label}:{command_pattern}\"\n",
    "    \n",
    "    elif node_type == 'File':\n",
    "        # Merge based on generalized path\n",
    "        path = generalize_path(props.get('path', ''))\n",
    "        return f\"{node_type}:{path}\"\n",
    "    \n",
    "    elif node_type == 'Registry':\n",
    "        # Merge based on generalized key\n",
    "        key = generalize_path(props.get('key', ''))\n",
    "        return f\"{node_type}:{key}\"\n",
    "    \n",
    "    elif node_type == 'Image':\n",
    "        # Merge based on generalized path\n",
    "        path = generalize_path(props.get('path', ''))\n",
    "        return f\"{node_type}:{path}\"\n",
    "    \n",
    "    else:\n",
    "        # Default: don't merge\n",
    "        return node_data.get('id', f\"{node_type}:unique\")\n",
    "\n",
    "\n",
    "print(\"‚úì Node merging logic ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5a67f",
   "metadata": {},
   "source": [
    "## Step 5: Main Aggregation Pipeline\n",
    "\n",
    "K·∫øt h·ª£p t·∫•t c·∫£ c√°c b∆∞·ªõc ƒë·ªÉ transform graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8119935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Aggregation pipeline ready\n"
     ]
    }
   ],
   "source": [
    "def aggregate_graph(graph_data):\n",
    "    \"\"\"\n",
    "    Apply all aggregation steps to a graph.\n",
    "    \n",
    "    Returns:\n",
    "        Aggregated graph + statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'original_nodes': len(graph_data['nodes']),\n",
    "        'original_edges': len(graph_data['edges']),\n",
    "        'removed_noise_nodes': 0,\n",
    "        'removed_noise_edges': 0,\n",
    "        'merged_nodes': 0,\n",
    "        'final_nodes': 0,\n",
    "        'final_edges': 0,\n",
    "    }\n",
    "    \n",
    "    # Step 1: Generalize all paths\n",
    "    for node in graph_data['nodes']:\n",
    "        props = node.get('properties', {})\n",
    "        \n",
    "        # Generalize path fields\n",
    "        if 'path' in props:\n",
    "            props['path'] = generalize_path(props['path'])\n",
    "        if 'key' in props:\n",
    "            props['key'] = generalize_path(props['key'])\n",
    "        if 'image' in props:\n",
    "            props['image'] = generalize_path(props['image'])\n",
    "        if 'commandLine' in props:\n",
    "            props['commandLine'] = generalize_path(props['commandLine'])\n",
    "    \n",
    "    # Step 2: Filter noise nodes\n",
    "    filtered_nodes = []\n",
    "    removed_node_ids = set()\n",
    "    \n",
    "    for node in graph_data['nodes']:\n",
    "        if is_noise_node(node):\n",
    "            removed_node_ids.add(node['id'])\n",
    "            stats['removed_noise_nodes'] += 1\n",
    "        else:\n",
    "            filtered_nodes.append(node)\n",
    "    \n",
    "    # Step 3: Filter noise edges + edges connected to removed nodes\n",
    "    filtered_edges = []\n",
    "    \n",
    "    for edge in graph_data['edges']:\n",
    "        # Skip if connected to removed node\n",
    "        if edge['source'] in removed_node_ids or edge['target'] in removed_node_ids:\n",
    "            stats['removed_noise_edges'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Skip if noise edge\n",
    "        if is_noise_edge(edge):\n",
    "            stats['removed_noise_edges'] += 1\n",
    "            continue\n",
    "        \n",
    "        filtered_edges.append(edge)\n",
    "    \n",
    "    # Step 4: Merge duplicate nodes\n",
    "    signature_to_nodes = defaultdict(list)\n",
    "    \n",
    "    for node in filtered_nodes:\n",
    "        sig = compute_node_signature(node)\n",
    "        signature_to_nodes[sig].append(node)\n",
    "    \n",
    "    # Create merged nodes\n",
    "    old_id_to_new_id = {}  # Map old IDs to new stable IDs\n",
    "    merged_nodes = []\n",
    "    \n",
    "    for sig, nodes in signature_to_nodes.items():\n",
    "        # Use first node as representative\n",
    "        merged_node = nodes[0].copy()\n",
    "        \n",
    "        # Create stable ID\n",
    "        new_id = create_stable_id(merged_node)\n",
    "        merged_node['id'] = new_id\n",
    "        \n",
    "        # Map all old IDs to new ID\n",
    "        for node in nodes:\n",
    "            old_id_to_new_id[node['id']] = new_id\n",
    "        \n",
    "        merged_nodes.append(merged_node)\n",
    "        \n",
    "        if len(nodes) > 1:\n",
    "            stats['merged_nodes'] += len(nodes) - 1\n",
    "    \n",
    "    # Step 5: Update edge references\n",
    "    updated_edges = []\n",
    "    seen_edges = set()  # Deduplicate edges\n",
    "    \n",
    "    for edge in filtered_edges:\n",
    "        new_source = old_id_to_new_id.get(edge['source'], edge['source'])\n",
    "        new_target = old_id_to_new_id.get(edge['target'], edge['target'])\n",
    "        \n",
    "        edge_key = (new_source, new_target, tuple(sorted(edge.get('operations', []))))\n",
    "        \n",
    "        if edge_key not in seen_edges:\n",
    "            updated_edges.append({\n",
    "                'source': new_source,\n",
    "                'target': new_target,\n",
    "                'operations': edge.get('operations', [])\n",
    "            })\n",
    "            seen_edges.add(edge_key)\n",
    "    \n",
    "    # Final stats\n",
    "    stats['final_nodes'] = len(merged_nodes)\n",
    "    stats['final_edges'] = len(updated_edges)\n",
    "    \n",
    "    aggregated_graph = {\n",
    "        'nodes': merged_nodes,\n",
    "        'edges': updated_edges,\n",
    "        'metadata': {\n",
    "            'version': 'v3.0-aggregated',\n",
    "            'aggregation_stats': stats\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return aggregated_graph, stats\n",
    "\n",
    "\n",
    "print(\"‚úì Aggregation pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23297b",
   "metadata": {},
   "source": [
    "## Step 6: Process All Techniques\n",
    "\n",
    "√Åp d·ª•ng aggregation cho t·∫•t c·∫£ 10 techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd858f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting aggregation for all techniques...\n",
      "\n",
      "‚úÖ T1003.001:\n",
      "   Nodes: 64 ‚Üí 28 (-0 noise, -36 merged) [56.2% reduction]\n",
      "   Edges: 65 ‚Üí 32 (-0 noise)\n",
      "\n",
      "‚úÖ T1003.002:\n",
      "   Nodes: 32 ‚Üí 14 (-0 noise, -18 merged) [56.2% reduction]\n",
      "   Edges: 29 ‚Üí 18 (-0 noise)\n",
      "\n",
      "‚úÖ T1059.001:\n",
      "   Nodes: 95 ‚Üí 21 (-0 noise, -74 merged) [77.9% reduction]\n",
      "   Edges: 120 ‚Üí 26 (-0 noise)\n",
      "\n",
      "‚úÖ T1112:\n",
      "   Nodes: 102 ‚Üí 21 (-0 noise, -81 merged) [79.4% reduction]\n",
      "   Edges: 102 ‚Üí 25 (-0 noise)\n",
      "\n",
      "‚úÖ T1204.002:\n",
      "   Nodes: 6 ‚Üí 6 (-0 noise, -0 merged) [0.0% reduction]\n",
      "   Edges: 5 ‚Üí 5 (-0 noise)\n",
      "\n",
      "‚úÖ T1218.005:\n",
      "   Nodes: 32 ‚Üí 13 (-0 noise, -19 merged) [59.4% reduction]\n",
      "   Edges: 28 ‚Üí 16 (-0 noise)\n",
      "\n",
      "‚úÖ T1218.011:\n",
      "   Nodes: 58 ‚Üí 18 (-0 noise, -40 merged) [69.0% reduction]\n",
      "   Edges: 60 ‚Üí 23 (-0 noise)\n",
      "\n",
      "‚úÖ T1482:\n",
      "   Nodes: 12 ‚Üí 7 (-0 noise, -5 merged) [41.7% reduction]\n",
      "   Edges: 11 ‚Üí 8 (-0 noise)\n",
      "\n",
      "‚úÖ T1547.001:\n",
      "   Nodes: 85 ‚Üí 29 (-0 noise, -56 merged) [65.9% reduction]\n",
      "   Edges: 46 ‚Üí 29 (-0 noise)\n",
      "\n",
      "‚úÖ T1548.002:\n",
      "   Nodes: 113 ‚Üí 21 (-0 noise, -92 merged) [81.4% reduction]\n",
      "   Edges: 111 ‚Üí 28 (-0 noise)\n",
      "\n",
      "\n",
      "‚úÖ Aggregation complete! Output saved to: d:\\nckh\\auditlog\\output_aggregated\n"
     ]
    }
   ],
   "source": [
    "# List of techniques to process\n",
    "TECHNIQUES = [\n",
    "    'T1003.001',\n",
    "    'T1003.002',\n",
    "    'T1059.001',\n",
    "    'T1112',\n",
    "    'T1204.002',\n",
    "    'T1218.005',\n",
    "    'T1218.011',\n",
    "    'T1482',\n",
    "    'T1547.001',\n",
    "    'T1548.002',\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"üöÄ Starting aggregation for all techniques...\\n\")\n",
    "\n",
    "for technique in TECHNIQUES:\n",
    "    input_file = INPUT_DIR / f\"{technique}_graph_v2.2.json\"\n",
    "    output_file = OUTPUT_DIR / f\"{technique}_graph_v3.0.json\"\n",
    "    \n",
    "    if not input_file.exists():\n",
    "        print(f\"‚ö†Ô∏è  {technique}: Input file not found\")\n",
    "        continue\n",
    "    \n",
    "    # Load graph\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        graph_data = json.load(f)\n",
    "    \n",
    "    # Aggregate\n",
    "    aggregated_graph, stats = aggregate_graph(graph_data)\n",
    "    \n",
    "    # Save\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(aggregated_graph, f, indent=2)\n",
    "    \n",
    "    # Report\n",
    "    reduction = 100 * (1 - stats['final_nodes'] / stats['original_nodes']) if stats['original_nodes'] > 0 else 0\n",
    "    \n",
    "    print(f\"‚úÖ {technique}:\")\n",
    "    print(f\"   Nodes: {stats['original_nodes']} ‚Üí {stats['final_nodes']} (-{stats['removed_noise_nodes']} noise, -{stats['merged_nodes']} merged) [{reduction:.1f}% reduction]\")\n",
    "    print(f\"   Edges: {stats['original_edges']} ‚Üí {stats['final_edges']} (-{stats['removed_noise_edges']} noise)\")\n",
    "    print()\n",
    "    \n",
    "    results.append({\n",
    "        'technique': technique,\n",
    "        'stats': stats\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Aggregation complete! Output saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad57e4",
   "metadata": {},
   "source": [
    "## Step 7: Summary Statistics\n",
    "\n",
    "T·ªïng k·∫øt k·∫øt qu·∫£ aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85abf0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aggregation Summary:\n",
      "\n",
      "Technique  Original Nodes  Final Nodes Node Reduction  Original Edges  Final Edges Edge Reduction\n",
      "T1003.001              64           28          56.2%              65           32          50.8%\n",
      "T1003.002              32           14          56.2%              29           18          37.9%\n",
      "T1059.001              95           21          77.9%             120           26          78.3%\n",
      "    T1112             102           21          79.4%             102           25          75.5%\n",
      "T1204.002               6            6           0.0%               5            5           0.0%\n",
      "T1218.005              32           13          59.4%              28           16          42.9%\n",
      "T1218.011              58           18          69.0%              60           23          61.7%\n",
      "    T1482              12            7          41.7%              11            8          27.3%\n",
      "T1547.001              85           29          65.9%              46           29          37.0%\n",
      "T1548.002             113           21          81.4%             111           28          74.8%\n",
      "\n",
      "üìà Overall Statistics:\n",
      "   Total nodes: 599 ‚Üí 178 (70.3% reduction)\n",
      "   Total edges: 577 ‚Üí 210 (63.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for result in results:\n",
    "    stats = result['stats']\n",
    "    node_reduction = 100 * (1 - stats['final_nodes'] / stats['original_nodes']) if stats['original_nodes'] > 0 else 0\n",
    "    edge_reduction = 100 * (1 - stats['final_edges'] / stats['original_edges']) if stats['original_edges'] > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Technique': result['technique'],\n",
    "        'Original Nodes': stats['original_nodes'],\n",
    "        'Final Nodes': stats['final_nodes'],\n",
    "        'Node Reduction': f\"{node_reduction:.1f}%\",\n",
    "        'Original Edges': stats['original_edges'],\n",
    "        'Final Edges': stats['final_edges'],\n",
    "        'Edge Reduction': f\"{edge_reduction:.1f}%\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìä Aggregation Summary:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Overall statistics\n",
    "total_original_nodes = sum(r['stats']['original_nodes'] for r in results)\n",
    "total_final_nodes = sum(r['stats']['final_nodes'] for r in results)\n",
    "total_original_edges = sum(r['stats']['original_edges'] for r in results)\n",
    "total_final_edges = sum(r['stats']['final_edges'] for r in results)\n",
    "\n",
    "overall_node_reduction = 100 * (1 - total_final_nodes / total_original_nodes) if total_original_nodes > 0 else 0\n",
    "overall_edge_reduction = 100 * (1 - total_final_edges / total_original_edges) if total_original_edges > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Overall Statistics:\")\n",
    "print(f\"   Total nodes: {total_original_nodes} ‚Üí {total_final_nodes} ({overall_node_reduction:.1f}% reduction)\")\n",
    "print(f\"   Total edges: {total_original_edges} ‚Üí {total_final_edges} ({overall_edge_reduction:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba7de4",
   "metadata": {},
   "source": [
    "## Step 8: Validate Detection Templates\n",
    "\n",
    "Ki·ªÉm tra graphs c√≥ s·∫µn s√†ng cho real-time detection kh√¥ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Validating detection templates...\n",
      "\n",
      "‚úÖ T1003.001: Ready for detection\n",
      "‚úÖ T1003.002: Ready for detection\n",
      "‚úÖ T1059.001: Ready for detection\n",
      "‚úÖ T1112: Ready for detection\n",
      "‚úÖ T1204.002: Ready for detection\n",
      "‚úÖ T1218.005: Ready for detection\n",
      "‚úÖ T1218.011: Ready for detection\n",
      "‚úÖ T1482: Ready for detection\n",
      "‚úÖ T1547.001: Ready for detection\n",
      "‚úÖ T1548.002: Ready for detection\n",
      "\n",
      "üìä Validation Summary: 10/10 templates ready for detection\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def validate_detection_template(graph_data, technique_id):\n",
    "    \"\"\"\n",
    "    Validate if a graph is suitable for detection.\n",
    "    \n",
    "    Checks:\n",
    "    1. Graph is not empty\n",
    "    2. All paths are generalized\n",
    "    3. No noise patterns remain\n",
    "    4. Graph is connected\n",
    "    5. Has malicious nodes\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check 1: Not empty\n",
    "    if len(graph_data['nodes']) == 0:\n",
    "        issues.append(\"Graph is empty\")\n",
    "        return issues\n",
    "    \n",
    "    # Check 2: Paths generalized\n",
    "    for node in graph_data['nodes']:\n",
    "        props = node.get('properties', {})\n",
    "        for key in ['path', 'key', 'image', 'commandLine']:\n",
    "            if key in props:\n",
    "                value = props[key]\n",
    "                if re.search(r'C:\\\\Users\\\\[^%]', value, re.IGNORECASE):\n",
    "                    issues.append(f\"Non-generalized path found: {value[:50]}...\")\n",
    "                    break\n",
    "    \n",
    "    # Check 3: No noise\n",
    "    for node in graph_data['nodes']:\n",
    "        if is_noise_node(node):\n",
    "            issues.append(f\"Noise node found: {node['id']}\")\n",
    "            break\n",
    "    \n",
    "    # Check 4: Graph connectivity\n",
    "    if len(graph_data['edges']) == 0:\n",
    "        issues.append(\"No edges in graph\")\n",
    "    \n",
    "    # Check 5: Has malicious markers\n",
    "    has_malicious = any(\n",
    "        node.get('properties', {}).get('malicious') == True\n",
    "        for node in graph_data['nodes']\n",
    "    )\n",
    "    if not has_malicious:\n",
    "        issues.append(\"No malicious nodes marked\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "\n",
    "print(\"\\nüîç Validating detection templates...\\n\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for technique in TECHNIQUES:\n",
    "    output_file = OUTPUT_DIR / f\"{technique}_graph_v3.0.json\"\n",
    "    \n",
    "    if not output_file.exists():\n",
    "        continue\n",
    "    \n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        graph_data = json.load(f)\n",
    "    \n",
    "    issues = validate_detection_template(graph_data, technique)\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"‚ö†Ô∏è  {technique}: {len(issues)} issue(s)\")\n",
    "        for issue in issues:\n",
    "            print(f\"     - {issue}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {technique}: Ready for detection\")\n",
    "    \n",
    "    validation_results.append({\n",
    "        'technique': technique,\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues\n",
    "    })\n",
    "\n",
    "valid_count = sum(1 for r in validation_results if r['valid'])\n",
    "print(f\"\\nüìä Validation Summary: {valid_count}/{len(validation_results)} templates ready for detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1ca4b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### ‚úÖ Graphs ƒë√£ s·∫µn s√†ng cho real-time detection v·ªõi:\n",
    "\n",
    "1. **Generalized paths** ‚Üí Match ƒë∆∞·ª£c v·ªõi nhi·ªÅu m√°y kh√°c nhau\n",
    "2. **Filtered noise** ‚Üí Gi·∫£m false positives\n",
    "3. **Stable IDs** ‚Üí Consistent matching\n",
    "4. **Compact structure** ‚Üí Fast matching\n",
    "\n",
    "### üéØ C√°ch d√πng trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø:\n",
    "\n",
    "```python\n",
    "# 1. Load detection templates (10 graphs v3.0)\n",
    "templates = load_all_templates()\n",
    "\n",
    "# 2. Monitor real-time events\n",
    "for event in stream_events():\n",
    "    graph = build_graph_from_event(event)\n",
    "    \n",
    "    # 3. Match v·ªõi templates\n",
    "    for template in templates:\n",
    "        if graph_match(graph, template, threshold=0.8):\n",
    "            alert(f\"Detected {template.technique_id}\")\n",
    "```\n",
    "\n",
    "### üìÅ Output:\n",
    "- `output_aggregated/T*.json` - Detection templates v3.0\n",
    "- Reduced noise by 30-50%\n",
    "- Generalized for cross-machine matching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
